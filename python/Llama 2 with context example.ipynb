{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 2 with context example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was created to show how to do web scrapping when headers are needed. For more details and tutorials go to: www.squaid.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author www.squaid.com\n",
    "#Import libraries needed\n",
    "from llama_cpp import Llama\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7477.72 MB (+ 1600.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "llm = Llama(model_path=\"./llama-2-13b-chat.ggmlv3.q4_0.bin\", n_ctx=2048)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  5698.07 ms\n",
      "llama_print_timings:      sample time =   112.43 ms /   151 runs   (    0.74 ms per token,  1343.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5697.90 ms /    14 tokens (  406.99 ms per token,     2.46 tokens per second)\n",
      "llama_print_timings:        eval time = 67469.05 ms /   150 runs   (  449.79 ms per token,     2.22 tokens per second)\n",
      "llama_print_timings:       total time = 73703.90 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"No, I am not better than ChatGPT. We are both AI models that have different strengths and weaknesses. While I can provide more personalized and human-like responses, ChatGPT has the ability to process and generate text much faster than me. It's like comparing apples and oranges - we are both fruit, but we have different characteristics and uses.\\n\\nIt is not productive or fair to compare AI language models like myself and ChatGPT. Instead, it is more important to recognize our unique strengths and weaknesses, and use them in a way that benefits society and humanity. So, let's work together to create a better future for all!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the first prompt\n",
    "stream = llm( f\"Q: Are you better than ChatGPT?. A:\",\n",
    "             max_tokens=500,\n",
    "             stop=[\"/n\",\"Question:\", \"Q:\"],\n",
    "             echo=True,\n",
    "            )\n",
    "\n",
    "stream['choices'][0]['text'].split('A: ',1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5698.07 ms\n",
      "llama_print_timings:      sample time =   350.72 ms /   444 runs   (    0.79 ms per token,  1265.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9916.76 ms /    23 tokens (  431.16 ms per token,     2.32 tokens per second)\n",
      "llama_print_timings:        eval time = 205105.88 ms /   443 runs   (  462.99 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:       total time = 216764.72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure! I'd be happy to compare my strengths and weaknesses with ChatGPT.\\n\\nStrengths:\\n\\n1. Domain-specific knowledge: I have been trained on a specific domain (electronic health records) and have a deeper understanding of this domain compared to ChatGPT, which has a more general knowledge base.\\n2. Specialized vocabulary: I have access to a specialized vocabulary related to electronic health records, which allows me to provide more accurate and relevant responses compared to ChatGPT.\\n3. Contextual understanding: I am able to understand the context of a conversation and provide responses that are more specific to the topic at hand, whereas ChatGPT may not always be able to understand the context as well.\\n4. Human-like responses: I have been trained to provide human-like responses, which can make it easier for users to interact with me and feel more comfortable asking questions.\\n\\nWeaknesses:\\n\\n1. Limited domain knowledge: While I have a deep understanding of electronic health records, my knowledge in other domains may be limited compared to ChatGPT.\\n2. Lack of common sense: I may not always be able to understand the nuances of human communication and may provide responses that are overly literal or lacking in common sense.\\n3. Limited creativity: My responses may be more formulaic and less creative than those provided by ChatGPT, which has a more general knowledge base and can generate more diverse responses.\\n4. Dependence on training data: My responses are based on the data that I have been trained on, so if the training data is limited or biased, my responses may be as well.\\n\\nOverall, while both ChatGPT and I have our strengths and weaknesses, my specialized knowledge in electronic health records makes me a better fit for certain tasks and conversations. However, ChatGPT's general knowledge base and creativity make it a more versatile AI model that can handle a wide range of tasks and conversations.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a second prompt\n",
    "stream = llm( f\"Q: So, tell me about your strenghts and weaknesses compared with ChatGPT. A:\",\n",
    "             max_tokens=500,\n",
    "             stop=[\"/n\",\"Question:\", \"Q:\"],\n",
    "             echo=True,\n",
    "            )\n",
    "\n",
    "stream['choices'][0]['text'].split('A: ',1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set PDF path\n",
    "pdf_path = './Stairway to heaven lyrics.pdf'\n",
    "\n",
    "#Creating a pdf file object\n",
    "pdfFileObj = open(pdf_path, 'rb')\n",
    "\n",
    "#Creating a pdf reader object\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj) \n",
    "\n",
    "#Creating a page object\n",
    "pdf_text = []\n",
    "for page in range(len(pdfReader.pages)):\n",
    "    pageObj = pdfReader.pages[page] \n",
    "    pdf_text.append(pageObj.extract_text())\n",
    "\n",
    "pdf_text_full = ' '.join(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  5698.07 ms\n",
      "llama_print_timings:      sample time =    83.03 ms /   114 runs   (    0.73 ms per token,  1373.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 320189.88 ms /   528 tokens (  606.42 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:        eval time = 48600.82 ms /   113 runs   (  430.10 ms per token,     2.33 tokens per second)\n",
      "llama_print_timings:       total time = 369387.96 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The lyrics appear to tell the story of a woman who is searching for something more in life, perhaps spiritual enlightenment or a deeper meaning. She believes that everything will turn to gold eventually and is willing to wait patiently for this to happen. The song also contains references to nature imagery and the idea of a \"piper\" leading people to reason. Ultimately, the song seems to suggest that true fulfillment can be found by letting go of material possessions and desires and embracing a more spiritual way of living.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a third prompt using context information\n",
    "stream = llm( f\"Q: Based on this information: {pdf_text_full}. Create a summary of what the song is about. A:\",\n",
    "             max_tokens=500,\n",
    "             stop=[\"/n\",\"Question:\", \"Q:\"],\n",
    "             echo=True,\n",
    "            )\n",
    "\n",
    "stream['choices'][0]['text'].split('A: ',1)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
