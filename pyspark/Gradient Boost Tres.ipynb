{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código es un ejemplo de cómo utilizar Spark y su librería de Machine Learning, PySpark ML, para entrenar y evaluar un modelo de clasificación de Gradient Boosted Trees en el conjunto de datos \"Iris\" de UCI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, importamos las librerías necesarias para trabajar con Spark y con los diferentes componentes que vamos a utilizar para entrenar y evaluar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí creamos una sesión de Spark con el nombre \"GBTClassifier\". Esta sesión nos permite trabajar con Spark y cargar, preparar y analizar grandes conjuntos de datos de manera distribuida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creamos una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"GBTClassifier\").getOrCreate()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, cargamos el conjunto de datos \"Iris\" de UCI en un dataframe de Spark. Los datos se encuentran en un archivo CSV en línea, y los leemos usando el método read de SparkSession y especificando el formato del archivo, si tiene encabezados y si se debe inferir el esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargamos el conjunto de datos (usaremos el conjunto de datos \"Iris\" de UCI)\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, seleccionamos las columnas que vamos a usar como variables de entrada en nuestro modelo y les asignamos nombres más descriptivos. Las columnas son \"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\" y \"species\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleccionamos las columnas necesarias y las renombramos\n",
    "data = data.selectExpr(\"_c0 as sepal_length\", \"_c1 as sepal_width\", \"_c2 as petal_length\", \"_c3 as petal_width\", \"_c4 as species\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, creamos un transformador VectorAssembler que convierte nuestras variables de entrada en un único vector de características, que se usará como entrada para nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertimos las variables de entrada en un vector de características\n",
    "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, convertimos nuestra variable objetivo \"species\" en un índice numérico usando el transformador StringIndexer. Esto nos permite usar la variable en nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertimos la variable objetivo en un índice numérico\n",
    "labelIndexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "data = labelIndexer.fit(data).transform(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, dividimos nuestro conjunto de datos en conjuntos de entrenamiento y prueba, con una proporción de 70% para entrenamiento y 30% para prueba. Usamos la función randomSplit de Spark y especificamos una semilla para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividimos el conjunto de datos en conjuntos de entrenamiento y prueba\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], seed=123)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se crea un modelo de Gradient Boosted Trees utilizando el GBTClassifier de la librería pyspark.ml.classification. labelCol especifica la columna que contiene los valores de la variable objetivo y featuresCol especifica la columna que contiene el vector de características. maxIter es el número máximo de iteraciones para el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creamos un modelo de Gradient Boosted Trees\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, el modelo de Gradient Boosted Trees se entrena utilizando los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entrenamos el modelo con el conjunto de entrenamiento\n",
    "model = gbt.fit(trainingData)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se utiliza el método transform para realizar predicciones en el conjunto de prueba utilizando el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtenemos las predicciones con el conjunto de prueba\n",
    "predictions = model.transform(testData)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, se utiliza MulticlassClassificationEvaluator de la librería pyspark.ml.evaluation para evaluar la precisión del modelo. labelCol especifica la columna que contiene los valores de la variable objetivo, predictionCol especifica la columna que contiene las predicciones y metricName especifica la métrica a utilizar para evaluar el modelo (en este caso, la precisión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluamos la precisión del modelo\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se imprime la metrica seleccionada para evaluar este modelo, que es la precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imprimimos la precisión del modelo\n",
    "print(\"GBTClassifier - Precisión: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se detiene la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detenemos la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
